%load_ext rpy2.ipython

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer

from sklearn.datasets import fetch_20newsgroups

remove = ('headers', 'footers', 'quotes')


def get_train_data(categories):
    if type(categories) is not list:
        categories = [categories]
    return fetch_20newsgroups(subset='train', shuffle=True, categories=categories, random_state=42, remove=remove)


all_categories = ['comp.graphics', 'rec.sport.baseball', 'sci.electronics']
train_bunch = get_train_data(all_categories)
test_bunch = fetch_20newsgroups(subset='test', shuffle=True, random_state=42, categories=all_categories, remove=remove)


def get_sample(bunch, category_idx):
    for idx, target in enumerate(bunch.target):
        if target == category_idx:
            return bunch.data[idx]

get_sample(train_bunch, all_categories.index('comp.graphics'))

I'm interested in simulating reverse (or negative) color video\nmathematically.  What is the transform?  Is it a simple\nreversal of the hue value in the HSV color space?  Is it\na manipulation in the YUV color space?  How is it related\nto solarization?\n\nIf you want to see something truly wild, turn on the\nreverse video effect on a camcorder so equipped,\nand point it at the monitor.  This creates a chaotic\ndynamical system whose phase space is continuous along\nrotation, zoom, focus, etc.  Very very surprising and \nlovely.  I'd like to write a simulation of this effect\nwithout analog grunge.  Thanks for any info you may have.\n\nPlease e-mail any info to me.  I'll post a summary.\n\nThanks,\n\n-- 

get_sample(train_bunch, all_categories.index('rec.sport.baseball'))

I had heard the rumors about LA, Cin, Hou, and SD all being\ninterested in Mark Davis, so it doesn\'t surprise me that a\nteam had to give up something and cash to actually get him.\n\nLynch "MOB"

get_sample(train_bunch, all_categories.index('sci.electronics'))

I had the insturment panel go out in my car (a 1990 Lincoln Contenintal) which\nis a digital dash. They replaced the whole thing with a 1991 dash (thank god it\nwas under the warrenty ! :-) Anyway, the odometer was reading the exact milage\nfrom the old panel. It must have a EEPROM of some sort in it that is up-dated.\nSeems to me that removing the battery would erase it, but it doesn't. So I\nguess they swapped the NVM chip (non-volitile memory) and installed it in the\nnew dash. No, they wouldn't let me have the old dash to tinker with :-(\n

import nltk
from nltk.stem import *
from nltk import word_tokenize

nltk.download('punkt')


def stemminize(documents: list[str]) -> list[str]:
    porter_stemmer = PorterStemmer()
    stem_train = []
    for document in documents:
        nltk_tokens = word_tokenize(document)
        line = ''
        for word in nltk_tokens:
            line += ' ' + porter_stemmer.stem(word)
        stem_train.append(line)
    return stem_train


train_tokenized = stemminize(train_bunch.data)
test_tokenized = stemminize(test_bunch.data)

train_tokenized[:3]

[10]
0 сек.
train_tokenized[:3]
output
[" i had the instur panel go out in my car ( a 1990 lincoln contenint ) which is a digit dash . they replac the whole thing with a 1991 dash ( thank god it wa under the warrenti ! : - ) anyway , the odomet wa read the exact milag from the old panel . it must have a eeprom of some sort in it that is up-dat . seem to me that remov the batteri would eras it , but it doe n't . so i guess they swap the nvm chip ( non-volitil memori ) and instal it in the new dash . no , they would n't let me have the old dash to tinker with : - (",
 " i had heard the rumor about la , cin , hou , and sd all be interest in mark davi , so it doe n't surpris me that a team had to give up someth and cash to actual get him . lynch `` mob ''",
 " i 'm interest in simul revers ( or neg ) color video mathemat . what is the transform ? is it a simpl revers of the hue valu in the hsv color space ? is it a manipul in the yuv color space ? how is it relat to solar ? if you want to see someth truli wild , turn on the revers video effect on a camcord so equip , and point it at the monitor . thi creat a chaotic dynam system whose phase space is continu along rotat , zoom , focu , etc . veri veri surpris and love . i 'd like to write a simul of thi effect without analog grung . thank for ani info you may have . pleas e-mail ani info to me . i 'll post a summari . thank , --"]

columns = pd.MultiIndex.from_product([['Count', 'TF', 'TF-IDF'], ['Без стоп-слов', 'С стоп-словами']])
df_train = pd.DataFrame(columns=columns)
df_test = pd.DataFrame(columns=columns)

df_train_stem = pd.DataFrame(columns=columns)
df_test_stem = pd.DataFrame(columns=columns)

vect = CountVectorizer(max_features=10000)
train_data = vect.fit_transform(train_bunch.data)


def get_20_freq_words(vect, data):
    words = list(zip(vect.get_feature_names_out(), np.ravel(data.sum(axis=0))))
    words.sort(key=lambda x: x[1], reverse=True)
    return words[:20]


count_column = get_20_freq_words(vect, train_data)
df_train['Count', 'Без стоп-слов'] = count_column
count_column


0 сек.
vect = CountVectorizer(max_features=10000)
train_data = vect.fit_transform(train_bunch.data)


def get_20_freq_words(vect, data):
    words = list(zip(vect.get_feature_names_out(), np.ravel(data.sum(axis=0))))
    words.sort(key=lambda x: x[1], reverse=True)
    return words[:20]


count_column = get_20_freq_words(vect, train_data)
df_train['Count', 'Без стоп-слов'] = count_column
count_column
output
[('the', 11217),
 ('to', 5625),
 ('and', 4649),
 ('of', 4275),
 ('is', 3512),
 ('in', 3298),
 ('for', 2692),
 ('it', 2578),
 ('that', 2456),
 ('you', 2210),
 ('on', 1742),
 ('this', 1636),
 ('be', 1581),
 ('have', 1505),
 ('with', 1494),
 ('are', 1410),
 ('or', 1393),
 ('if', 1301),
 ('as', 1189),
 ('but', 1182)]

def get_20_freq_words_idf(feature_names, tfidf_values):
    result = []
    word_weights = dict(zip(feature_names, tfidf_values))
    sorted_words = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)
    for word, weight in sorted_words[:20]:
        result.append((word, weight))
    return result


vectorizer = CountVectorizer(max_features=10000)
dtm = vectorizer.fit_transform(train_bunch.data)
tfidf = TfidfTransformer(use_idf=False).fit_transform(dtm)

feature_names = vectorizer.get_feature_names_out()
tfidf_values = tfidf.toarray().sum(axis=0)

tf_column = get_20_freq_words_idf(feature_names, tfidf_values)
df_train['TF', 'Без стоп-слов'] = tf_column
tf_column


1 сек.
def get_20_freq_words_idf(feature_names, tfidf_values):
    result = []
    word_weights = dict(zip(feature_names, tfidf_values))
    sorted_words = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)
    for word, weight in sorted_words[:20]:
        result.append((word, weight))
    return result


vectorizer = CountVectorizer(max_features=10000)
dtm = vectorizer.fit_transform(train_bunch.data)
tfidf = TfidfTransformer(use_idf=False).fit_transform(dtm)

feature_names = vectorizer.get_feature_names_out()
tfidf_values = tfidf.toarray().sum(axis=0)

tf_column = get_20_freq_words_idf(feature_names, tfidf_values)
df_train['TF', 'Без стоп-слов'] = tf_column
tf_column
output
[('the', 533.4922807849437),
 ('to', 275.8748829930202),
 ('and', 210.74679041879125),
 ('of', 203.27421480192316),
 ('in', 168.05567850652892),
 ('is', 165.6458215892349),
 ('for', 139.50376330402037),
 ('it', 137.5933837024795),
 ('that', 128.1830973532055),
 ('you', 106.7291231981497),
 ('on', 90.99388015313521),
 ('have', 87.11801374241186),
 ('this', 86.68927655273733),
 ('be', 77.9877263998729),
 ('with', 70.49674206221073),
 ('if', 69.45979671449946),
 ('are', 68.55843840506661),
 ('or', 66.36867298924544),
 ('but', 65.53939178170177),
 ('can', 57.68500973851248)]

def randomForest(x, y, test_size = 0.25, n_estimators = 100):
  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = test_size, random_state = 1)
  rfc = RandomForestClassifier(n_estimators = n_estimators)
  rfc.fit(x_train, y_train)
  prediction = rfc.predict(x_test)

  print ('Prediction and test:')
  print ('Prediction: \t', prediction)
  print ('Test: \t\t', y_test)
  print("\n\n")
  print ('Confusion matrix: ')
  print (confusion_matrix(y_test, prediction)[0])
  print (confusion_matrix(y_test, prediction)[1])
  print("\n\n")
  print ('Accuracy score: ', accuracy_score(prediction, y_test))
  print("\n\n")
  print('Classification Report\n', classification_report(y_test, prediction))
  print("\n\n")
  print('ROC AUC')
  print(roc_auc_score(y_test, prediction))
  print("\n\n")

  # обучающая и тестовая выборки
  plt.title('Division into training (Blue) and test (Red) samples')
  plt.scatter (x_train[:, 0], x_train[:, 1], color = 'blue')
  plt.scatter (x_test[:, 0], x_test[:, 1], color = 'red')
  plt.show()

  plt.xlabel("first feature")
  plt.ylabel("second feature")
  plot_2d_separator(rfc, x, fill=True)
  plt.scatter(x[:, 0], x[:, 1], c=y, s=70)


vectorizer = CountVectorizer(max_features=10000, stop_words='english')
dtm = vectorizer.fit_transform(test_bunch.data)
tfidf = TfidfTransformer(use_idf=True).fit_transform(dtm)

feature_names = vectorizer.get_feature_names_out()
tfidf_values = tfidf.toarray().sum(axis=0)

tf_idf_stop_test = get_20_freq_words_idf(feature_names, tfidf_values)
df_test['TF-IDF', 'С стоп-словами'] = tf_idf_stop_test
tf_idf_stop_test

[('know', 21.58985409837126),
 ('like', 19.0432238776313),
 ('thanks', 17.243050835722823),
 ('does', 17.21331485116614),
 ('just', 16.893197620659922),
 ('don', 16.827878554078698),
 ('think', 15.961774996967483),
 ('graphics', 14.152144747044698),
 ('time', 14.027722341198748),
 ('game', 14.003738256027455),
 ('use', 13.74412905058512),
 ('edu', 12.529370396463287),
 ('program', 12.14189017243623),
 ('good', 11.781849219177335),
 ('need', 11.678500624748112),
 ('ve', 11.055266820963013),
 ('image', 10.88628915030409),
 ('help', 10.859661452807238),
 ('year', 10.809326893529198),
 ('games', 10.683050419564378)]

vect = CountVectorizer(max_features=10000)
dtm = vect.fit_transform(test_bunch.data)
count_test_column = get_20_freq_words(vect, dtm)
df_test['Count', 'Без стоп-слов'] = count_test_column
count_test_column

[('the', 8332),
 ('to', 4798),
 ('and', 3807),
 ('of', 3645),
 ('is', 2849),
 ('in', 2645),
 ('for', 2218),
 ('it', 2150),
 ('that', 2058),
 ('you', 1972),
 ('on', 1380),
 ('this', 1343),
 ('be', 1276),
 ('with', 1219),
 ('or', 1176),
 ('have', 1165),
 ('if', 1080),
 ('are', 1073),
 ('as', 1015),
 ('can', 995)]

vect = CountVectorizer(max_features=10000, stop_words='english')
dtm = vect.fit_transform(test_bunch.data)
count_column_stop = get_20_freq_words(vect, dtm)
df_test['Count', 'С стоп-словами'] = count_column_stop
count_column_stop

[('image', 668),
 ('jpeg', 526),
 ('edu', 490),
 ('graphics', 462),
 ('don', 396),
 ('like', 390),
 ('file', 378),
 ('use', 353),
 ('know', 328),
 ('just', 327),
 ('data', 311),
 ('images', 306),
 ('time', 299),
 ('available', 297),
 ('software', 297),
 ('does', 279),
 ('bit', 275),
 ('format', 268),
 ('program', 265),
 ('think', 260)]

vectorizer = CountVectorizer(max_features=10000, stop_words='english')
dtm = vectorizer.fit_transform(test_tokenized)
tfidf = TfidfTransformer(use_idf=False).fit_transform(dtm)

feature_names = vectorizer.get_feature_names_out()
tfidf_values = tfidf.toarray().sum(axis=0)

tf_column_stem_stop_test = get_20_freq_words_idf(feature_names, tfidf_values)
df_test_stem['TF', 'С стоп-словами'] = tf_column_stem_stop_test
tf_column_stem_stop_test

vectorizer = CountVectorizer(max_features=10000)
dtm = vectorizer.fit_transform(test_tokenized)
tfidf = TfidfTransformer(use_idf=True).fit_transform(dtm)

feature_names = vectorizer.get_feature_names_out()
tfidf_values = tfidf.toarray().sum(axis=0)

tf_idf_column_stem_test = get_20_freq_words_idf(feature_names, tfidf_values)
df_test_stem['TF-IDF', 'Без стоп-слов'] = tf_idf_column_stem_test
tf_idf_column_stem_test

[('the', 125.95198360340036),
 ('to', 79.63102193960454),
 ('of', 59.34763415735523),
 ('and', 57.54416178268257),
 ('is', 52.06682745523498),
 ('it', 51.13475816632351),
 ('in', 49.34680609018263),
 ('that', 46.35095982697217),
 ('you', 42.394678612279655),
 ('for', 41.72483189921012),
 ('be', 32.835553889032155),
 ('have', 32.33709291827443),
 ('on', 32.151766696037676),
 ('thi', 31.988208550382435),
 ('if', 28.795631565452148),
 ('or', 28.541588560610176),
 ('he', 27.09299126865356),
 ('but', 26.872775861371704),
 ('not', 26.634026800244765),
 ('can', 26.452253299458658)]

vectorizer = CountVectorizer(max_features=10000, stop_words='english')
dtm = vectorizer.fit_transform(test_tokenized)
tfidf = TfidfTransformer(use_idf=False).fit_transform(dtm)

feature_names = vectorizer.get_feature_names_out()
tfidf_values = tfidf.toarray().sum(axis=0)

tf_idf_column_stem_stop_test = get_20_freq_words_idf(feature_names, tfidf_values)
df_test_stem['TF-IDF', 'С стоп-словами'] = tf_idf_column_stem_stop_test
tf_idf_column_stem_stop_test

[('thi', 89.8581665408178),
 ('wa', 54.60753122299347),
 ('use', 51.17031002579643),
 ('ani', 42.47881902913168),
 ('know', 42.44153125295944),
 ('like', 36.781387616616406),
 ('ha', 36.53835938568995),
 ('doe', 35.05526044372548),
 ('game', 31.729028939949327),
 ('hi', 30.552574621820913),
 ('just', 29.796080238519064),
 ('thank', 29.013340404340745),
 ('anyon', 27.83832283407521),
 ('think', 27.2693612032798),
 ('look', 26.331612633938164),
 ('time', 25.254159608989454),
 ('work', 23.513389368539453),
 ('need', 22.814503817326177),
 ('pleas', 22.408021336751386),
 ('year', 22.27522855693658)]